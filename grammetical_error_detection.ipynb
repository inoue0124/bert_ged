{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "grammetical_error_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOuSsC8qi+WsF64ZxBZzNpv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7df55f8ddc3d4b4fa45b6b7c7ade34e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1d71d7eb952e4ac8b25fc69b49687c14",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_89df0d34552849b79ad9c46cd5412dfc",
              "IPY_MODEL_ee33d5b70a1747bcb2c2db81dde350be"
            ]
          }
        },
        "1d71d7eb952e4ac8b25fc69b49687c14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89df0d34552849b79ad9c46cd5412dfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c8baae5d2a1240b9b181210db15457b4",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 257706,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 257706,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3e974d88054e4c54ba0f75ec2499db83"
          }
        },
        "ee33d5b70a1747bcb2c2db81dde350be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d74c77cf33b04274ada86b3cae834113",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 258k/258k [00:00&lt;00:00, 3.10MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_58d3d2fbf2e44b3fa3ff2764759804c9"
          }
        },
        "c8baae5d2a1240b9b181210db15457b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3e974d88054e4c54ba0f75ec2499db83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d74c77cf33b04274ada86b3cae834113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "58d3d2fbf2e44b3fa3ff2764759804c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f32dacb67fd49b7a94b4af5b14f3cb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7b1e38d6c44447b0948e4a9da036a196",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_be9d01173d464f58a870ed6ff8932e96",
              "IPY_MODEL_e4ca170cc1e74c9c97a76ca861c520d9"
            ]
          }
        },
        "7b1e38d6c44447b0948e4a9da036a196": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be9d01173d464f58a870ed6ff8932e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a6ec2ff9853c4eef8c65c6b71d916c57",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 479,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 479,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_efd2bad460334dc0b2a9421a276889be"
          }
        },
        "e4ca170cc1e74c9c97a76ca861c520d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e5ae01417b3741c58061dc8eca4d2dc1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 479/479 [00:45&lt;00:00, 10.6B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3529ded8d6eb42c7a804f0be2bb9ff7f"
          }
        },
        "a6ec2ff9853c4eef8c65c6b71d916c57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "efd2bad460334dc0b2a9421a276889be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e5ae01417b3741c58061dc8eca4d2dc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3529ded8d6eb42c7a804f0be2bb9ff7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f74b9dd7ac0f434491ebd00db6630491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ca4431f032e44ab0b4efac6e8f56ddc2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9735ce8381f946b7b041d8d691d40d85",
              "IPY_MODEL_53255af1397a416ba87f0a54f862ed90"
            ]
          }
        },
        "ca4431f032e44ab0b4efac6e8f56ddc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9735ce8381f946b7b041d8d691d40d85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_87d380f1c61f4c92846eb3f9de6bf9e0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 445021143,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 445021143,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3a2a99123f834e71a0ac4983c5d3e6bc"
          }
        },
        "53255af1397a416ba87f0a54f862ed90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fd019a0299fd4b46bf6e3f4e365cc9d0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 445M/445M [00:13&lt;00:00, 32.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_85ae75e89e424fa3af2dc445300b59d4"
          }
        },
        "87d380f1c61f4c92846eb3f9de6bf9e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3a2a99123f834e71a0ac4983c5d3e6bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd019a0299fd4b46bf6e3f4e365cc9d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "85ae75e89e424fa3af2dc445300b59d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inoue0124/bert_ged/blob/main/grammetical_error_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNEmGRIsiA9M",
        "outputId": "ca2d4c6d-cfb8-479c-aa7e-4889d6f25ac8"
      },
      "source": [
        "# 乱数シードの固定\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED_VALUE = 1234  # これはなんでも良い\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
        "random.seed(SEED_VALUE)\n",
        "np.random.seed(SEED_VALUE)\n",
        "torch.manual_seed(SEED_VALUE)  # PyTorchを使う場合"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9d8bcada50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZyPTWyHiCQy",
        "outputId": "46bbe004-5e8e-4dfb-ed4a-b577b63027e9"
      },
      "source": [
        "# GPUの使用確認：True or False\n",
        "torch.cuda.is_available()\n",
        "\n",
        "# TrueならGPU使用可能"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16KTguTPiUCh",
        "outputId": "b9069772-2d7b-4bd1-ea7c-682b5f08e491"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lubZCJYIjkrx",
        "outputId": "a73f37cf-991d-4df5-c505-da7f2b0e0f56"
      },
      "source": [
        "# MeCabとtransformersの用意\n",
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "# 以下で報告があるようにmecab-python3のバージョンを0.996.5にしないとtokezerで落ちる\n",
        "# https://stackoverflow.com/questions/62860717/huggingface-for-japanese-tokenizer\n",
        "!pip install mecab-python3==0.996.5\n",
        "!pip install unidic-lite # これないとMeCab実行時にエラーで落ちる\n",
        "!pip install transformers==2.9.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aptitude-common libcgi-fast-perl libcgi-pm-perl libclass-accessor-perl\n",
            "  libcwidget3v5 libencode-locale-perl libfcgi-perl libhtml-parser-perl\n",
            "  libhtml-tagset-perl libhttp-date-perl libhttp-message-perl libio-html-perl\n",
            "  libio-string-perl liblwp-mediatypes-perl libparse-debianchangelog-perl\n",
            "  libsigc++-2.0-0v5 libsub-name-perl libtimedate-perl liburi-perl libxapian30\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  aptitude-doc-en | aptitude-doc apt-xapian-index debtags tasksel\n",
            "  libcwidget-dev libdata-dump-perl libhtml-template-perl libxml-simple-perl\n",
            "  libwww-perl xapian-tools swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  aptitude aptitude-common libcgi-fast-perl libcgi-pm-perl\n",
            "  libclass-accessor-perl libcwidget3v5 libencode-locale-perl libfcgi-perl\n",
            "  libhtml-parser-perl libhtml-tagset-perl libhttp-date-perl\n",
            "  libhttp-message-perl libio-html-perl libio-string-perl\n",
            "  liblwp-mediatypes-perl libparse-debianchangelog-perl libsigc++-2.0-0v5\n",
            "  libsub-name-perl libtimedate-perl liburi-perl libxapian30 swig swig3.0\n",
            "0 upgraded, 23 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 4,978 kB of archives.\n",
            "After this operation, 21.4 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude-common all 0.8.10-6ubuntu1 [1,014 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigc++-2.0-0v5 amd64 2.10.0-2 [10.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcwidget3v5 amd64 0.5.17-7 [286 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxapian30 amd64 1.4.5-1ubuntu0.1 [631 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude amd64 0.8.10-6ubuntu1 [1,269 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-pm-perl all 4.38-1 [185 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfcgi-perl amd64 0.78-2build1 [32.8 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-fast-perl all 1:2.13-1 [9,940 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsub-name-perl amd64 0.21-1build1 [11.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libclass-accessor-perl all 0.51-1 [21.2 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-string-perl all 1.08-3 [11.1 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libparse-debianchangelog-perl all 1.2.0-12 [49.5 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 4,978 kB in 1s (4,533 kB/s)\n",
            "Selecting previously unselected package aptitude-common.\n",
            "(Reading database ... 160772 files and directories currently installed.)\n",
            "Preparing to unpack .../00-aptitude-common_0.8.10-6ubuntu1_all.deb ...\n",
            "Unpacking aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libsigc++-2.0-0v5:amd64.\n",
            "Preparing to unpack .../01-libsigc++-2.0-0v5_2.10.0-2_amd64.deb ...\n",
            "Unpacking libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Selecting previously unselected package libcwidget3v5:amd64.\n",
            "Preparing to unpack .../02-libcwidget3v5_0.5.17-7_amd64.deb ...\n",
            "Unpacking libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Selecting previously unselected package libxapian30:amd64.\n",
            "Preparing to unpack .../03-libxapian30_1.4.5-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Selecting previously unselected package aptitude.\n",
            "Preparing to unpack .../04-aptitude_0.8.10-6ubuntu1_amd64.deb ...\n",
            "Unpacking aptitude (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libhtml-tagset-perl.\n",
            "Preparing to unpack .../05-libhtml-tagset-perl_3.20-3_all.deb ...\n",
            "Unpacking libhtml-tagset-perl (3.20-3) ...\n",
            "Selecting previously unselected package liburi-perl.\n",
            "Preparing to unpack .../06-liburi-perl_1.73-1_all.deb ...\n",
            "Unpacking liburi-perl (1.73-1) ...\n",
            "Selecting previously unselected package libhtml-parser-perl.\n",
            "Preparing to unpack .../07-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n",
            "Unpacking libhtml-parser-perl (3.72-3build1) ...\n",
            "Selecting previously unselected package libcgi-pm-perl.\n",
            "Preparing to unpack .../08-libcgi-pm-perl_4.38-1_all.deb ...\n",
            "Unpacking libcgi-pm-perl (4.38-1) ...\n",
            "Selecting previously unselected package libfcgi-perl.\n",
            "Preparing to unpack .../09-libfcgi-perl_0.78-2build1_amd64.deb ...\n",
            "Unpacking libfcgi-perl (0.78-2build1) ...\n",
            "Selecting previously unselected package libcgi-fast-perl.\n",
            "Preparing to unpack .../10-libcgi-fast-perl_1%3a2.13-1_all.deb ...\n",
            "Unpacking libcgi-fast-perl (1:2.13-1) ...\n",
            "Selecting previously unselected package libsub-name-perl.\n",
            "Preparing to unpack .../11-libsub-name-perl_0.21-1build1_amd64.deb ...\n",
            "Unpacking libsub-name-perl (0.21-1build1) ...\n",
            "Selecting previously unselected package libclass-accessor-perl.\n",
            "Preparing to unpack .../12-libclass-accessor-perl_0.51-1_all.deb ...\n",
            "Unpacking libclass-accessor-perl (0.51-1) ...\n",
            "Selecting previously unselected package libencode-locale-perl.\n",
            "Preparing to unpack .../13-libencode-locale-perl_1.05-1_all.deb ...\n",
            "Unpacking libencode-locale-perl (1.05-1) ...\n",
            "Selecting previously unselected package libtimedate-perl.\n",
            "Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\n",
            "Selecting previously unselected package libhttp-date-perl.\n",
            "Preparing to unpack .../15-libhttp-date-perl_6.02-1_all.deb ...\n",
            "Unpacking libhttp-date-perl (6.02-1) ...\n",
            "Selecting previously unselected package libio-html-perl.\n",
            "Preparing to unpack .../16-libio-html-perl_1.001-1_all.deb ...\n",
            "Unpacking libio-html-perl (1.001-1) ...\n",
            "Selecting previously unselected package liblwp-mediatypes-perl.\n",
            "Preparing to unpack .../17-liblwp-mediatypes-perl_6.02-1_all.deb ...\n",
            "Unpacking liblwp-mediatypes-perl (6.02-1) ...\n",
            "Selecting previously unselected package libhttp-message-perl.\n",
            "Preparing to unpack .../18-libhttp-message-perl_6.14-1_all.deb ...\n",
            "Unpacking libhttp-message-perl (6.14-1) ...\n",
            "Selecting previously unselected package libio-string-perl.\n",
            "Preparing to unpack .../19-libio-string-perl_1.08-3_all.deb ...\n",
            "Unpacking libio-string-perl (1.08-3) ...\n",
            "Selecting previously unselected package libparse-debianchangelog-perl.\n",
            "Preparing to unpack .../20-libparse-debianchangelog-perl_1.2.0-12_all.deb ...\n",
            "Unpacking libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Selecting previously unselected package swig3.0.\n",
            "Preparing to unpack .../21-swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../22-swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up libhtml-tagset-perl (3.20-3) ...\n",
            "Setting up libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up libencode-locale-perl (1.05-1) ...\n",
            "Setting up libtimedate-perl (2.3000-2) ...\n",
            "Setting up libio-html-perl (1.001-1) ...\n",
            "Setting up aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Setting up liblwp-mediatypes-perl (6.02-1) ...\n",
            "Setting up liburi-perl (1.73-1) ...\n",
            "Setting up libhtml-parser-perl (3.72-3build1) ...\n",
            "Setting up libcgi-pm-perl (4.38-1) ...\n",
            "Setting up libio-string-perl (1.08-3) ...\n",
            "Setting up libsub-name-perl (0.21-1build1) ...\n",
            "Setting up libfcgi-perl (0.78-2build1) ...\n",
            "Setting up libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Setting up libclass-accessor-perl (0.51-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Setting up libhttp-date-perl (6.02-1) ...\n",
            "Setting up libcgi-fast-perl (1:2.13-1) ...\n",
            "Setting up libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Setting up libhttp-message-perl (6.14-1) ...\n",
            "Setting up libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Setting up aptitude (0.8.10-6ubuntu1) ...\n",
            "update-alternatives: using /usr/bin/aptitude-curses to provide /usr/bin/aptitude (aptitude) in auto mode\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.8)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.13)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.8)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.13)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "The following NEW packages will be installed:\n",
            "  file libmagic-mgc{a} libmagic1{a} libmecab-dev libmecab2{a} mecab mecab-ipadic{a} mecab-ipadic-utf8 mecab-jumandic{a} mecab-jumandic-utf8{a} mecab-utils{a} \n",
            "0 packages upgraded, 11 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 29.3 MB of archives. After unpacking 282 MB will be used.\n",
            "Get: 1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get: 2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get: 3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n",
            "Get: 4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n",
            "Get: 5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n",
            "Get: 6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n",
            "Get: 7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic-utf8 all 7.0-20130310-4 [16.2 MB]\n",
            "Get: 8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic all 7.0-20130310-4 [2,212 B]\n",
            "Get: 9 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n",
            "Get: 10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n",
            "Get: 11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n",
            "Fetched 29.3 MB in 2s (19.0 MB/s)\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 162022 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../01-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../02-file_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "Preparing to unpack .../03-libmecab2_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-5) ...\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "Preparing to unpack .../04-libmecab-dev_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-5) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../05-mecab-utils_0.996-5_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-5) ...\n",
            "Selecting previously unselected package mecab-jumandic-utf8.\n",
            "Preparing to unpack .../06-mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-jumandic.\n",
            "Preparing to unpack .../07-mecab-jumandic_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "Preparing to unpack .../08-mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../09-mecab_0.996-5_amd64.deb ...\n",
            "Unpacking mecab (0.996-5) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../10-mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Setting up libmecab2:amd64 (0.996-5) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up mecab-utils (0.996-5) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up libmecab-dev (0.996-5) ...\n",
            "Setting up file (1:5.32-2ubuntu0.4) ...\n",
            "Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Compiling Juman dictionary for Mecab.\n",
            "reading /usr/share/mecab/dic/juman/unk.def ... 37\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n",
            "reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n",
            "reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n",
            "reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n",
            "reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n",
            "reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n",
            "reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n",
            "reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n",
            "reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n",
            "reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n",
            "reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n",
            "reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n",
            "reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n",
            "reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n",
            "reading /usr/share/mecab/dic/juman/Special.csv ... 158\n",
            "reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-5) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-jumandic (7.0-20130310-4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "                            \n",
            "Collecting mecab-python3==0.996.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/62/f1e4ffba2f904b8998da8df4372d70cf4dad6301b94f4f9e50e9aea1b82e/mecab_python3-0.996.5-cp37-cp37m-manylinux2010_x86_64.whl (17.1MB)\n",
            "\u001b[K     |████████████████████████████████| 17.1MB 189kB/s \n",
            "\u001b[?25hInstalling collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.996.5\n",
            "Collecting unidic-lite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/2b/8cf7514cb57d028abcef625afa847d60ff1ffbf0049c36b78faa7c35046f/unidic-lite-1.0.8.tar.gz (47.4MB)\n",
            "\u001b[K     |████████████████████████████████| 47.4MB 64kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: unidic-lite\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-cp37-none-any.whl size=47658838 sha256=975eff2e68de6a00459051216d2b89ed148195b40d88a72f3c56b07d10687fa4\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/48/8d/b66d8361a27f58f41ec86640e4fd2640de0403a6367511eab7\n",
            "Successfully built unidic-lite\n",
            "Installing collected packages: unidic-lite\n",
            "Successfully installed unidic-lite-1.0.8\n",
            "Collecting transformers==2.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 29.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 46.5MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/59/bb06dd5ca53547d523422d32735585493e0103c992a52a97ba3aa3be33bf/tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 48.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.9.0) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.9.0) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.9.0) (2.10)\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.7.0 transformers-2.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy6K0kM2jmjI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "7df55f8ddc3d4b4fa45b6b7c7ade34e5",
            "1d71d7eb952e4ac8b25fc69b49687c14",
            "89df0d34552849b79ad9c46cd5412dfc",
            "ee33d5b70a1747bcb2c2db81dde350be",
            "c8baae5d2a1240b9b181210db15457b4",
            "3e974d88054e4c54ba0f75ec2499db83",
            "d74c77cf33b04274ada86b3cae834113",
            "58d3d2fbf2e44b3fa3ff2764759804c9"
          ]
        },
        "outputId": "66ffd854-4e1f-4367-c978-a1f7e84da8af"
      },
      "source": [
        "import torch\n",
        "import torchtext.legacy as torchtext  # torchtextを使用\n",
        "from transformers.modeling_bert import BertModel\n",
        "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
        "\n",
        "# 日本語BERTの分かち書き用tokenizerです\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(\n",
        "    'bert-base-japanese-whole-word-masking')\n",
        "\n",
        "max_length = 512  # 東北大学_日本語版の最大の単語数（サブワード数）は512\n",
        "\n",
        "def tokenizer_512(input_text):\n",
        "    \"\"\"torchtextのtokenizerとして扱えるように、512単語のpytorchでのencodeを定義。ここで[0]を指定し忘れないように\"\"\"\n",
        "    return tokenizer.encode(input_text, max_length=512, return_tensors='pt')[0]\n",
        "\n",
        "\n",
        "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_512, use_vocab=False, lower=False,\n",
        "                            include_lengths=True, batch_first=True, fix_length=max_length, pad_token=0)\n",
        "# 注意：tokenize=tokenizer.encodeと、.encodeをつけます。padding[PAD]のindexが0なので、0を指定します。\n",
        "\n",
        "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7df55f8ddc3d4b4fa45b6b7c7ade34e5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=257706.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK3SIg7Rk-ky"
      },
      "source": [
        "dataset_train_eval, dataset_test = torchtext.data.TabularDataset.splits(\n",
        "    path='.', train='/content/drive/My Drive/train.tsv', test='/content/drive/My Drive/test.tsv', format='tsv', fields=[('Text', TEXT), ('Label', LABEL)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tGxAIeav_TD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c9485e-f6b3-419b-a274-1a45bdeb111e"
      },
      "source": [
        "dataset_train, dataset_eval = dataset_train_eval.split(\n",
        "    split_ratio=1.0 - 1475/5901, random_state=random.seed(1234))\n",
        "\n",
        "# データを絞る\n",
        "dataset_train, _ = dataset_train.split(\n",
        "    split_ratio=100000/2968824, random_state=random.seed(1234))\n",
        "dataset_eval, _ = dataset_eval.split(\n",
        "    split_ratio=10000/989385, random_state=random.seed(1234))\n",
        "\n",
        "print(dataset_train.__len__())\n",
        "print(dataset_eval.__len__())\n",
        "print(dataset_test.__len__())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000\n",
            "10000\n",
            "53697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4fbD6qTwN3i"
      },
      "source": [
        "batch_size = 32  # BERTでは16、32あたりを使用する\n",
        "\n",
        "dl_train = torchtext.data.Iterator(\n",
        "    dataset_train, batch_size=batch_size, train=True)\n",
        "\n",
        "dl_eval = torchtext.data.Iterator(\n",
        "    dataset_eval, batch_size=batch_size, train=False, sort=False)\n",
        "\n",
        "dl_test = torchtext.data.Iterator(\n",
        "    dataset_test, batch_size=batch_size, train=False, sort=False)\n",
        "\n",
        "# 辞書オブジェクトにまとめる\n",
        "dataloaders_dict = {\"train\": dl_train, \"val\": dl_eval}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3g6RV14wThU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c09e0071-7847-43be-867d-1a82697bc358"
      },
      "source": [
        "# DataLoaderの動作確認 \n",
        "\n",
        "batch = next(iter(dl_test))\n",
        "print(batch)\n",
        "print(batch.Text[0].shape)\n",
        "print(batch.Label.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[torchtext.legacy.data.batch.Batch of size 32]\n",
            "\t[.Text]:('[torch.LongTensor of size 32x512]', '[torch.LongTensor of size 32]')\n",
            "\t[.Label]:[torch.LongTensor of size 32]\n",
            "torch.Size([32, 512])\n",
            "torch.Size([32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-15-rJMeyBgU"
      },
      "source": [
        "# BERTのクラス分類用のモデルを用意する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLxNRe6UyLe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2f32dacb67fd49b7a94b4af5b14f3cb2",
            "7b1e38d6c44447b0948e4a9da036a196",
            "be9d01173d464f58a870ed6ff8932e96",
            "e4ca170cc1e74c9c97a76ca861c520d9",
            "a6ec2ff9853c4eef8c65c6b71d916c57",
            "efd2bad460334dc0b2a9421a276889be",
            "e5ae01417b3741c58061dc8eca4d2dc1",
            "3529ded8d6eb42c7a804f0be2bb9ff7f",
            "f74b9dd7ac0f434491ebd00db6630491",
            "ca4431f032e44ab0b4efac6e8f56ddc2",
            "9735ce8381f946b7b041d8d691d40d85",
            "53255af1397a416ba87f0a54f862ed90",
            "87d380f1c61f4c92846eb3f9de6bf9e0",
            "3a2a99123f834e71a0ac4983c5d3e6bc",
            "fd019a0299fd4b46bf6e3f4e365cc9d0",
            "85ae75e89e424fa3af2dc445300b59d4"
          ]
        },
        "outputId": "c9ce3e0b-86d5-4e40-cfab-e4ac04ab9477"
      },
      "source": [
        "from transformers.modeling_bert import BertModel\n",
        "\n",
        "# BERTの日本語学習済みパラメータのモデルです\n",
        "model = BertModel.from_pretrained('bert-base-japanese-whole-word-masking')\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f32dacb67fd49b7a94b4af5b14f3cb2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=479.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f74b9dd7ac0f434491ebd00db6630491",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=445021143.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "BertModel(\n",
            "  (embeddings): BertEmbeddings(\n",
            "    (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
            "    (position_embeddings): Embedding(512, 768)\n",
            "    (token_type_embeddings): Embedding(2, 768)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): BertEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (8): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (9): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (10): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (11): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pooler): BertPooler(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (activation): Tanh()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt8X6P2Dz35Z"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class BertForLivedoor(nn.Module):\n",
        "    '''BERTモデルにLivedoorニュースの9クラスを判定する部分をつなげたモデル'''\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BertForLivedoor, self).__init__()\n",
        "\n",
        "        # BERTモジュール\n",
        "        self.bert = model  # 日本語学習済みのBERTモデル\n",
        "\n",
        "        # headにポジネガ予測を追加\n",
        "        # 入力はBERTの出力特徴量の次元768、出力は2クラス\n",
        "        self.cls = nn.Linear(in_features=768, out_features=2)\n",
        "\n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.cls.weight, std=0.02)\n",
        "        nn.init.normal_(self.cls.bias, 0)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        '''\n",
        "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
        "        '''\n",
        "\n",
        "        # BERTの基本モデル部分の順伝搬\n",
        "        # 順伝搬させる\n",
        "        result = self.bert(input_ids)  # reult は、sequence_output, pooled_output\n",
        "\n",
        "        # sequence_outputの先頭の単語ベクトルを抜き出す\n",
        "        vec_0 = result[0]  # 最初の0がsequence_outputを示す\n",
        "        vec_0 = vec_0[:, 0, :]  # 全バッチ。先頭0番目の単語の全768要素\n",
        "        vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_size]に変換\n",
        "        output = self.cls(vec_0)  # 全結合層\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQN_lRpFz8Lk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba845420-5c31-4a68-da09-2fa62d16659b"
      },
      "source": [
        "net = BertForLivedoor()\n",
        "\n",
        "# 訓練モードに設定\n",
        "net.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForLivedoor(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (cls): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYLwnOwF0Yd6"
      },
      "source": [
        "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
        "\n",
        "# 1. まず全部を、勾配計算Falseにしてしまう\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2. BertLayerモジュールの最後を勾配計算ありに変更\n",
        "for param in net.bert.encoder.layer[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 3. 識別器を勾配計算ありに変更\n",
        "for param in net.cls.parameters():\n",
        "    param.requires_grad = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESfsPHd70Z7x"
      },
      "source": [
        "# 最適化手法の設定\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# BERTの元の部分はファインチューニング\n",
        "optimizer = optim.Adam([\n",
        "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
        "    {'params': net.cls.parameters(), 'lr': 1e-6}\n",
        "])\n",
        "\n",
        "# 損失関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Nnm3NgZ0GiQ"
      },
      "source": [
        "# 訓練を実施"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkDVvDFX0ARK"
      },
      "source": [
        "# モデルを学習させる関数を作成\n",
        "\n",
        "\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "    print('-----start-------')\n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # ミニバッチのサイズ\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "            else:\n",
        "                net.eval()   # モデルを検証モードに\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            epoch_corrects = 0  # epochの正解数\n",
        "            iteration = 1\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                # batchはTextとLableの辞書型変数\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                inputs = batch.Text[0].to(device)  # 文章\n",
        "                labels = batch.Label.to(device)  # ラベル\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    # BERTに入力\n",
        "                    outputs = net(inputs)\n",
        "\n",
        "                    loss = criterion(outputs, labels)  # 損失を計算\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
        "                            acc = (torch.sum(preds == labels.data)\n",
        "                                   ).double()/batch_size\n",
        "                            print('イテレーション {} || Loss: {:.4f} || 10iter. || 本イテレーションの正解率：{}'.format(\n",
        "                                iteration, loss.item(),  acc))\n",
        "\n",
        "                    iteration += 1\n",
        "\n",
        "                    # 損失と正解数の合計を更新\n",
        "                    epoch_loss += loss.item() * batch_size\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # epochごとのlossと正解率\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double(\n",
        "            ) / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
        "                                                                           phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdkGzeAi0Q6S",
        "outputId": "d7d10277-1de6-4f80-d95f-59665b6f0ad6"
      },
      "source": [
        "num_epochs = 4\n",
        "net_trained = train_model(net, dataloaders_dict,\n",
        "                          criterion, optimizer, num_epochs=num_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "使用デバイス： cuda:0\n",
            "-----start-------\n",
            "イテレーション 10 || Loss: 0.4463 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 20 || Loss: 0.4386 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 30 || Loss: 0.5462 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 40 || Loss: 0.7098 || 10iter. || 本イテレーションの正解率：0.65625\n",
            "イテレーション 50 || Loss: 0.4641 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 60 || Loss: 0.4574 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 70 || Loss: 0.6572 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 80 || Loss: 0.5671 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 90 || Loss: 0.5279 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 100 || Loss: 0.6871 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 110 || Loss: 0.5715 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 120 || Loss: 0.4710 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 130 || Loss: 0.6063 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 140 || Loss: 0.5015 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 150 || Loss: 0.5553 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 160 || Loss: 0.4280 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 170 || Loss: 0.5716 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 180 || Loss: 0.4905 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 190 || Loss: 0.4905 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 200 || Loss: 0.5978 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 210 || Loss: 0.4308 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 220 || Loss: 0.5467 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 230 || Loss: 0.6344 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 240 || Loss: 0.5318 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 250 || Loss: 0.4724 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 260 || Loss: 0.4842 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 270 || Loss: 0.5069 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 280 || Loss: 0.5176 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 290 || Loss: 0.5954 || 10iter. || 本イテレーションの正解率：0.65625\n",
            "イテレーション 300 || Loss: 0.4505 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 310 || Loss: 0.5866 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 320 || Loss: 0.4412 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 330 || Loss: 0.5391 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 340 || Loss: 0.6970 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 350 || Loss: 0.6386 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 360 || Loss: 0.4385 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 370 || Loss: 0.3581 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 380 || Loss: 0.6711 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 390 || Loss: 0.5561 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 400 || Loss: 0.4007 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 410 || Loss: 0.6253 || 10iter. || 本イテレーションの正解率：0.59375\n",
            "イテレーション 420 || Loss: 0.4241 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 430 || Loss: 0.5128 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 440 || Loss: 0.4012 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 450 || Loss: 0.3799 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 460 || Loss: 0.6369 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 470 || Loss: 0.5729 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 480 || Loss: 0.4588 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 490 || Loss: 0.4633 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 500 || Loss: 0.4493 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 510 || Loss: 0.5368 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 520 || Loss: 0.5941 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 530 || Loss: 0.5068 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 540 || Loss: 0.5403 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 550 || Loss: 0.4830 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 560 || Loss: 0.5896 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 570 || Loss: 0.4592 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 580 || Loss: 0.4861 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 590 || Loss: 0.4711 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 600 || Loss: 0.5971 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 610 || Loss: 0.6327 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 620 || Loss: 0.3407 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 630 || Loss: 0.5034 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 640 || Loss: 0.4781 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 650 || Loss: 0.5196 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 660 || Loss: 0.6950 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 670 || Loss: 0.4548 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 680 || Loss: 0.4526 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 690 || Loss: 0.4409 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 700 || Loss: 0.5047 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 710 || Loss: 0.5380 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 720 || Loss: 0.4303 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 730 || Loss: 0.3765 || 10iter. || 本イテレーションの正解率：0.90625\n",
            "イテレーション 740 || Loss: 0.5086 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 750 || Loss: 0.4273 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 760 || Loss: 0.5480 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 770 || Loss: 0.3160 || 10iter. || 本イテレーションの正解率：0.90625\n",
            "イテレーション 780 || Loss: 0.4361 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 790 || Loss: 0.4757 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 800 || Loss: 0.4669 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 810 || Loss: 0.6510 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 820 || Loss: 0.3538 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 830 || Loss: 0.4814 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 840 || Loss: 0.4962 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 850 || Loss: 0.4978 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 860 || Loss: 0.6021 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 870 || Loss: 0.6364 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 880 || Loss: 0.5688 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 890 || Loss: 0.5608 || 10iter. || 本イテレーションの正解率：0.65625\n",
            "イテレーション 900 || Loss: 0.4631 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 910 || Loss: 0.4774 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 920 || Loss: 0.8460 || 10iter. || 本イテレーションの正解率：0.53125\n",
            "イテレーション 930 || Loss: 0.4482 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 940 || Loss: 0.4370 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 950 || Loss: 0.5401 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 960 || Loss: 0.4126 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 970 || Loss: 0.5079 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 980 || Loss: 0.5299 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 990 || Loss: 0.4175 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1000 || Loss: 0.5447 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1010 || Loss: 0.3770 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 1020 || Loss: 0.5181 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1030 || Loss: 0.6541 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1040 || Loss: 0.4796 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1050 || Loss: 0.3863 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1060 || Loss: 0.3090 || 10iter. || 本イテレーションの正解率：0.90625\n",
            "イテレーション 1070 || Loss: 0.5426 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1080 || Loss: 0.5444 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 1090 || Loss: 0.4042 || 10iter. || 本イテレーションの正解率：0.90625\n",
            "イテレーション 1100 || Loss: 0.5207 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1110 || Loss: 0.5228 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1120 || Loss: 0.5863 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 1130 || Loss: 0.7370 || 10iter. || 本イテレーションの正解率：0.5625\n",
            "イテレーション 1140 || Loss: 0.5432 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1150 || Loss: 0.7241 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 1160 || Loss: 0.5674 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 1170 || Loss: 0.5694 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1180 || Loss: 0.5929 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1190 || Loss: 0.3257 || 10iter. || 本イテレーションの正解率：0.90625\n",
            "イテレーション 1200 || Loss: 0.5638 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1210 || Loss: 0.4578 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1220 || Loss: 0.7282 || 10iter. || 本イテレーションの正解率：0.59375\n",
            "イテレーション 1230 || Loss: 0.5574 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1240 || Loss: 0.4347 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1250 || Loss: 0.4825 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1260 || Loss: 0.5038 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1270 || Loss: 0.3819 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1280 || Loss: 0.5115 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1290 || Loss: 0.4901 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1300 || Loss: 0.5156 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1310 || Loss: 0.4395 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1320 || Loss: 0.5374 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1330 || Loss: 0.3947 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1340 || Loss: 0.6020 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 1350 || Loss: 0.4384 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1360 || Loss: 0.5356 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1370 || Loss: 0.4799 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1380 || Loss: 0.4643 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1390 || Loss: 0.3829 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1400 || Loss: 0.4172 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1410 || Loss: 0.6153 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 1420 || Loss: 0.5427 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1430 || Loss: 0.5313 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1440 || Loss: 0.5159 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1450 || Loss: 0.5163 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1460 || Loss: 0.5591 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 1470 || Loss: 0.4520 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1480 || Loss: 0.4243 || 10iter. || 本イテレーションの正解率：0.90625\n",
            "イテレーション 1490 || Loss: 0.6126 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1500 || Loss: 0.5724 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 1510 || Loss: 0.6245 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 1520 || Loss: 0.5377 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1530 || Loss: 0.4780 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1540 || Loss: 0.4193 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1550 || Loss: 0.4700 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1560 || Loss: 0.2781 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 1570 || Loss: 0.5248 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 1580 || Loss: 0.5392 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1590 || Loss: 0.5866 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1600 || Loss: 0.6424 || 10iter. || 本イテレーションの正解率：0.59375\n",
            "イテレーション 1610 || Loss: 0.4779 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1620 || Loss: 0.6960 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 1630 || Loss: 0.6079 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1640 || Loss: 0.6025 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1650 || Loss: 0.4850 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1660 || Loss: 0.4564 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1670 || Loss: 0.4116 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1680 || Loss: 0.5750 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1690 || Loss: 0.3741 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 1700 || Loss: 0.5254 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1710 || Loss: 0.5209 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1720 || Loss: 0.4837 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1730 || Loss: 0.2960 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 1740 || Loss: 0.5619 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1750 || Loss: 0.4138 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 1760 || Loss: 0.6281 || 10iter. || 本イテレーションの正解率：0.65625\n",
            "イテレーション 1770 || Loss: 0.7788 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 1780 || Loss: 0.7225 || 10iter. || 本イテレーションの正解率：0.59375\n",
            "イテレーション 1790 || Loss: 0.5292 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1800 || Loss: 0.2797 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 1810 || Loss: 0.2504 || 10iter. || 本イテレーションの正解率：0.96875\n",
            "イテレーション 1820 || Loss: 0.5432 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1830 || Loss: 0.3407 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 1840 || Loss: 0.6252 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1850 || Loss: 0.5732 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1860 || Loss: 0.5193 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1870 || Loss: 0.5156 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1880 || Loss: 0.4599 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1890 || Loss: 0.4414 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1900 || Loss: 0.3639 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1910 || Loss: 0.4770 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1920 || Loss: 0.6126 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 1930 || Loss: 0.5473 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1940 || Loss: 0.5021 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1950 || Loss: 0.5023 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1960 || Loss: 0.5515 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1970 || Loss: 0.4532 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1980 || Loss: 0.5510 || 10iter. || 本イテレーションの正解率：0.65625\n",
            "イテレーション 1990 || Loss: 0.4443 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 2000 || Loss: 0.5170 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2010 || Loss: 0.5719 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 2020 || Loss: 0.4433 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2030 || Loss: 0.5205 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 2040 || Loss: 0.4757 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 2050 || Loss: 0.5287 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 2060 || Loss: 0.6104 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 2070 || Loss: 0.3963 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 2080 || Loss: 0.4633 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 2090 || Loss: 0.6032 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2100 || Loss: 0.5655 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 2110 || Loss: 0.4140 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 2120 || Loss: 0.4751 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 2130 || Loss: 0.4017 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 2140 || Loss: 0.4382 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 2150 || Loss: 0.4513 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 2160 || Loss: 0.5651 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 2170 || Loss: 0.4575 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 2180 || Loss: 0.4506 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 2190 || Loss: 0.4411 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 2200 || Loss: 0.3846 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 2210 || Loss: 0.5875 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 2220 || Loss: 0.4433 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 2230 || Loss: 0.4585 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2240 || Loss: 0.6920 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 2250 || Loss: 0.5202 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 2260 || Loss: 0.4658 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 2270 || Loss: 0.4632 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 2280 || Loss: 0.6301 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 2290 || Loss: 0.4843 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2300 || Loss: 0.3084 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 2310 || Loss: 0.4585 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 2320 || Loss: 0.3044 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 2330 || Loss: 0.4185 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 2340 || Loss: 0.5104 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 2350 || Loss: 0.4505 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2360 || Loss: 0.6451 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 2370 || Loss: 0.4018 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 2380 || Loss: 0.6344 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 2390 || Loss: 0.4550 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 2400 || Loss: 0.5446 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 2410 || Loss: 0.5617 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 2420 || Loss: 0.5382 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 2430 || Loss: 0.6056 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 2440 || Loss: 0.5286 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 2450 || Loss: 0.5247 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 2460 || Loss: 0.5893 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 2470 || Loss: 0.5226 || 10iter. || 本イテレーションの正解率：0.65625\n",
            "イテレーション 2480 || Loss: 0.4400 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 2490 || Loss: 0.3968 || 10iter. || 本イテレーションの正解率：0.90625\n",
            "イテレーション 2500 || Loss: 0.6857 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 2510 || Loss: 0.5105 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 2520 || Loss: 0.4838 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 2530 || Loss: 0.5374 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2540 || Loss: 0.4920 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 2550 || Loss: 0.4577 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2560 || Loss: 0.5152 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2570 || Loss: 0.4929 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2580 || Loss: 0.5186 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2590 || Loss: 0.5494 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 2600 || Loss: 0.4970 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 2610 || Loss: 0.5960 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 2620 || Loss: 0.4177 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2630 || Loss: 0.3563 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 2640 || Loss: 0.5161 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 2650 || Loss: 0.4912 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2660 || Loss: 0.6239 || 10iter. || 本イテレーションの正解率：0.65625\n",
            "イテレーション 2670 || Loss: 0.5486 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2680 || Loss: 0.4732 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2690 || Loss: 0.5971 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 2700 || Loss: 0.5477 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 2710 || Loss: 0.6340 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 2720 || Loss: 0.5848 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 2730 || Loss: 0.3366 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 2740 || Loss: 0.6635 || 10iter. || 本イテレーションの正解率：0.65625\n",
            "イテレーション 2750 || Loss: 0.4117 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 2760 || Loss: 0.6233 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 2770 || Loss: 0.5406 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 2780 || Loss: 0.3566 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 2790 || Loss: 0.4561 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2800 || Loss: 0.5775 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2810 || Loss: 0.6806 || 10iter. || 本イテレーションの正解率：0.59375\n",
            "イテレーション 2820 || Loss: 0.5741 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 2830 || Loss: 0.5473 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2840 || Loss: 0.4347 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 2850 || Loss: 0.5193 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 2860 || Loss: 0.5376 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 2870 || Loss: 0.4608 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 2880 || Loss: 0.4337 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 2890 || Loss: 0.6484 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 2900 || Loss: 0.5922 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 2910 || Loss: 0.6431 || 10iter. || 本イテレーションの正解率：0.5625\n",
            "イテレーション 2920 || Loss: 0.6591 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 2930 || Loss: 0.6038 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 2940 || Loss: 0.5757 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 2950 || Loss: 0.4578 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 2960 || Loss: 0.5407 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 2970 || Loss: 0.6063 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 2980 || Loss: 0.5863 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 2990 || Loss: 0.6692 || 10iter. || 本イテレーションの正解率：0.65625\n",
            "イテレーション 3000 || Loss: 0.5911 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 3010 || Loss: 0.6069 || 10iter. || 本イテレーションの正解率：0.65625\n",
            "イテレーション 3020 || Loss: 0.5948 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 3030 || Loss: 0.4599 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 3040 || Loss: 0.7566 || 10iter. || 本イテレーションの正解率：0.5625\n",
            "イテレーション 3050 || Loss: 0.5098 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 3060 || Loss: 0.5556 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 3070 || Loss: 0.5191 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 3080 || Loss: 0.6510 || 10iter. || 本イテレーションの正解率：0.65625\n",
            "イテレーション 3090 || Loss: 0.5152 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 3100 || Loss: 0.4824 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 3110 || Loss: 0.6274 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 3120 || Loss: 0.4740 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "Epoch 1/4 | train |  Loss: 0.5099 Acc: 0.7652\n",
            "Epoch 1/4 |  val  |  Loss: 0.5056 Acc: 0.7657\n",
            "イテレーション 10 || Loss: 0.3993 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 20 || Loss: 0.5154 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 30 || Loss: 0.3460 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 40 || Loss: 0.4967 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 50 || Loss: 0.3913 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 60 || Loss: 0.6081 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 70 || Loss: 0.4548 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 80 || Loss: 0.5148 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 90 || Loss: 0.4614 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 100 || Loss: 0.5062 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 110 || Loss: 0.7149 || 10iter. || 本イテレーションの正解率：0.65625\n",
            "イテレーション 120 || Loss: 0.5609 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 130 || Loss: 0.5522 || 10iter. || 本イテレーションの正解率：0.65625\n",
            "イテレーション 140 || Loss: 0.4186 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 150 || Loss: 0.3835 || 10iter. || 本イテレーションの正解率：0.90625\n",
            "イテレーション 160 || Loss: 0.3641 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 170 || Loss: 0.6028 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 180 || Loss: 0.4854 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 190 || Loss: 0.4137 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 200 || Loss: 0.3896 || 10iter. || 本イテレーションの正解率：0.90625\n",
            "イテレーション 210 || Loss: 0.3873 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 220 || Loss: 0.4025 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 230 || Loss: 0.4141 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 240 || Loss: 0.6773 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 250 || Loss: 0.4137 || 10iter. || 本イテレーションの正解率：0.90625\n",
            "イテレーション 260 || Loss: 0.4807 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 270 || Loss: 0.3590 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 280 || Loss: 0.4552 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 290 || Loss: 0.4925 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 300 || Loss: 0.5218 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 310 || Loss: 0.5235 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 320 || Loss: 0.3396 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 330 || Loss: 0.4914 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 340 || Loss: 0.4733 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 350 || Loss: 0.5872 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 360 || Loss: 0.3691 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 370 || Loss: 0.4724 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 380 || Loss: 0.4116 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 390 || Loss: 0.3678 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 400 || Loss: 0.4811 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 410 || Loss: 0.6135 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 420 || Loss: 0.3766 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 430 || Loss: 0.4984 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 440 || Loss: 0.5228 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 450 || Loss: 0.4718 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 460 || Loss: 0.4624 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 470 || Loss: 0.4165 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 480 || Loss: 0.5947 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 490 || Loss: 0.5204 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 500 || Loss: 0.4123 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 510 || Loss: 0.5936 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 520 || Loss: 0.4600 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 530 || Loss: 0.5650 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 540 || Loss: 0.4460 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 550 || Loss: 0.5850 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 560 || Loss: 0.6696 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 570 || Loss: 0.3369 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 580 || Loss: 0.4266 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 590 || Loss: 0.4857 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 600 || Loss: 0.5024 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 610 || Loss: 0.6201 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 620 || Loss: 0.4877 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 630 || Loss: 0.5569 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 640 || Loss: 0.4246 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 650 || Loss: 0.4287 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 660 || Loss: 0.5914 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 670 || Loss: 0.5343 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 680 || Loss: 0.5303 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 690 || Loss: 0.4253 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 700 || Loss: 0.6280 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 710 || Loss: 0.5969 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 720 || Loss: 0.5775 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 730 || Loss: 0.3675 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 740 || Loss: 0.4474 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 750 || Loss: 0.5155 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 760 || Loss: 0.5792 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 770 || Loss: 0.5727 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 780 || Loss: 0.5828 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 790 || Loss: 0.3907 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 800 || Loss: 0.4248 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 810 || Loss: 0.6607 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 820 || Loss: 0.4397 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 830 || Loss: 0.4956 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 840 || Loss: 0.3112 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 850 || Loss: 0.5463 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 860 || Loss: 0.3764 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 870 || Loss: 0.4134 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 880 || Loss: 0.4988 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 890 || Loss: 0.4782 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 900 || Loss: 0.3963 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 910 || Loss: 0.4335 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 920 || Loss: 0.5318 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 930 || Loss: 0.4364 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 940 || Loss: 0.5342 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 950 || Loss: 0.4205 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 960 || Loss: 0.8056 || 10iter. || 本イテレーションの正解率：0.65625\n",
            "イテレーション 970 || Loss: 0.4936 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 980 || Loss: 0.5372 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 990 || Loss: 0.5903 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1000 || Loss: 0.6695 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 1010 || Loss: 0.3930 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1020 || Loss: 0.6432 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1030 || Loss: 0.4226 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1040 || Loss: 0.4482 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1050 || Loss: 0.5268 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 1060 || Loss: 0.4759 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1070 || Loss: 0.5202 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 1080 || Loss: 0.4789 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1090 || Loss: 0.3576 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 1100 || Loss: 0.3784 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 1110 || Loss: 0.3911 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 1120 || Loss: 0.3696 || 10iter. || 本イテレーションの正解率：0.90625\n",
            "イテレーション 1130 || Loss: 0.4194 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1140 || Loss: 0.5186 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1150 || Loss: 0.4672 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1160 || Loss: 0.3234 || 10iter. || 本イテレーションの正解率：0.90625\n",
            "イテレーション 1170 || Loss: 0.4463 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1180 || Loss: 0.4999 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1190 || Loss: 0.4829 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1200 || Loss: 0.5287 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 1210 || Loss: 0.4983 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1220 || Loss: 0.4715 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1230 || Loss: 0.5526 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1240 || Loss: 0.4670 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1250 || Loss: 0.6032 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1260 || Loss: 0.5087 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1270 || Loss: 0.5926 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1280 || Loss: 0.4860 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1290 || Loss: 0.4505 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1300 || Loss: 0.5023 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1310 || Loss: 0.4987 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1320 || Loss: 0.4710 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1330 || Loss: 0.3957 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1340 || Loss: 0.5031 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1350 || Loss: 0.3873 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1360 || Loss: 0.5344 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1370 || Loss: 0.6212 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1380 || Loss: 0.4903 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1390 || Loss: 0.4521 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1400 || Loss: 0.5430 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1410 || Loss: 0.4991 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1420 || Loss: 0.4877 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1430 || Loss: 0.5251 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1440 || Loss: 0.6146 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 1450 || Loss: 0.5330 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1460 || Loss: 0.5619 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1470 || Loss: 0.4498 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1480 || Loss: 0.4642 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1490 || Loss: 0.5862 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1500 || Loss: 0.4819 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1510 || Loss: 0.5824 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1520 || Loss: 0.4302 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1530 || Loss: 0.6343 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 1540 || Loss: 0.4649 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1550 || Loss: 0.5975 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1560 || Loss: 0.6524 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 1570 || Loss: 0.4592 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1580 || Loss: 0.5890 || 10iter. || 本イテレーションの正解率：0.71875\n",
            "イテレーション 1590 || Loss: 0.4652 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1600 || Loss: 0.3967 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 1610 || Loss: 0.5761 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1620 || Loss: 0.3861 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 1630 || Loss: 0.5583 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1640 || Loss: 0.3216 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 1650 || Loss: 0.4862 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1660 || Loss: 0.4500 || 10iter. || 本イテレーションの正解率：0.78125\n",
            "イテレーション 1670 || Loss: 0.5331 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 1680 || Loss: 0.4401 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 1690 || Loss: 0.4781 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1700 || Loss: 0.5685 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 1710 || Loss: 0.4494 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1720 || Loss: 0.3980 || 10iter. || 本イテレーションの正解率：0.84375\n",
            "イテレーション 1730 || Loss: 0.3398 || 10iter. || 本イテレーションの正解率：0.90625\n",
            "イテレーション 1740 || Loss: 0.5346 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 1750 || Loss: 0.4159 || 10iter. || 本イテレーションの正解率：0.84375\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}